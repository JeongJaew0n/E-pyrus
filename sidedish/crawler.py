import os
from bs4 import BeautifulSoup
import cloudscraper
import requests;
import time

folder_path = "results"
# file_path = os.path.join(folder_path, file_name)

proxy_url = "54.248.238.110:80"

# ëª©ë¡ ìŠ¤ìº”
def getListHtml(url, headers, saveFile:bool = False):
    response = requests.get(url, headers=headers)

    if response.ok:
        soup = BeautifulSoup(response.text, "html.parser")

        # mkdirì²˜ëŸ¼ ë™ì‘. exist_okê°€ falseì¼ ê²½ìš° í´ë”ê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ì—ëŸ¬ ë°œìƒ. defaultê°’ì´ falseì„.
        os.makedirs(folder_path, exist_ok=True)
        result = soup.prettify()
        if saveFile:
            list_path = os.path.join(folder_path, "list.html")
            with open(list_path, "w") as file:
                file.write(result)
            print(f"File created at: {list_path}")

        print("List crawling completed âœ…")
        return result

    else:
        print(f"List crawling failed âŒ: {response.status_code} ")

# íŒŒì¼ ì½ê¸°
def readFile(path):
    with open(path, "r") as file:
        content = file.read()
    return content

class Chapter:
    def __init__(self, title: str, link: str):
        self.title = title
        self.link = link

    def __str__(self):
        return f"{self.title}: {self.link}\n"
    
    def __iter__(self):
        return iter((self.title, self.link))

# todo: novel_pathê°€ globalì¸ ê±´ ì¢‹ì§€ ì•ŠìŒ.
novel_path: str
# list htmlì„ íŒŒì‹±í•´ì„œ ë§í¬ì™€ ì œëª©ì„ ë¹ˆí™˜.
def parseList(listHtml):
    soup = listHtml if isinstance(listHtml, BeautifulSoup) else BeautifulSoup(listHtml, "html.parser")

    # ë…¸ë²¨ ì œëª©
    title = soup.select_one(".view-title .view-content span b").get_text(strip=True)
    global novel_path
    novel_path = os.path.join(folder_path, title)
    os.makedirs(novel_path, exist_ok=True)
    
    subject_a_tags = soup.select(".wr-subject a")
    chapter_list: list[Chapter] = []
    for a_tag in subject_a_tags:
        # ì œëª©ë§Œ ë‚¨ê¸°ê¸° ìœ„í•´ span íƒœê·¸ ì‚­ì œ
        [span.decompose() for span in a_tag.select("span")]
        chapter_list.append(Chapter(a_tag.get_text(strip=True), a_tag.get("href")))
    return chapter_list

# Chapter í•˜ë‚˜ë¥¼ ìŠ¤ìº”.
def scanCapter(ch: Chapter, headers):
    title, link = ch

    response = requests.get(link, headers=headers)
    if response.ok:
        soup = BeautifulSoup(response.text, "html.parser")
        with open(f"{os.path.join(novel_path, title)}.txt", "w") as file:
            novel_soup = soup.select_one('#novel_content')
            for a in novel_soup.find_all("p"):
                a.insert_after("\n")
                a.insert_after("\n")
                a.unwrap()
            file.write(novel_soup.getText().strip())
        print(f"{title} âœ…" )

    else:
        print(f"Chapter crawling failed âŒ: title:{title} | status code: {response.status_code} ")

def scanAllChapter(chapter_list: list[Chapter], headers, time_wait = 5, waitFunction: callable = None):
        for ch in chapter_list:
            scanCapter(ch, headers=headers)
            if waitFunction:
                waitFunction()
            else:
                time.sleep(time_wait)
            
        print("All chapter crawling completed ğŸš€")


# Chapter ìŠ¤ìº” í…ŒìŠ¤íŠ¸ìš©
def scanCapterTest(ch: Chapter):
    title, link = ch

    file = readFile("TestChapter.html")
    
    soup = BeautifulSoup(file, "html.parser")
    with open(f"{os.path.join("results/", title)}.txt", "w") as file:
        novel_soup = soup.select_one('#novel_content')
        for a in novel_soup.find_all("p"):
            a.insert_after("\n")
            a.insert_after("\n")
            a.unwrap()
        file.write(novel_soup.getText().strip())
    print(f"{title} âœ…" )